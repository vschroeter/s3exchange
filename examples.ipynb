{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Exchange Library - Examples\n",
    "\n",
    "This notebook demonstrates how to use the `s3-exchange` library for managing S3 artifacts with manifest-based data exchange.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Initialization](#setup)\n",
    "2. [Basic Object Operations](#basic-ops)\n",
    "3. [Manifest Operations](#manifests)\n",
    "4. [Shard Archives](#shards)\n",
    "5. [ManifestWriter - Incremental Writing](#manifest-writer)\n",
    "6. [Listing and Filtering](#listing)\n",
    "7. [Deletion Operations](#deletion)\n",
    "8. [Error Handling](#errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store initialized with bucket: wakeworx-artifacts\n",
      "Base prefix: prod\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "from s3_exchange import S3ExchangeStore\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "S3_ACCESS_KEY_ID = os.getenv(\"S3_AWS_ACCESS_KEY_ID\")\n",
    "S3_SECRET_ACCESS_KEY = os.getenv(\"S3_AWS_SECRET_ACCESS_KEY\")\n",
    "S3_REGION = os.getenv(\"S3_REGION\")\n",
    "S3_ENDPOINT_URL = os.getenv(\"S3_ENDPOINT_URL\")\n",
    "S3_BUCKET = os.getenv(\"S3_BUCKET\")\n",
    "\n",
    "# Initialize S3 client\n",
    "# For local development (e.g., Garage), use endpoint_url\n",
    "# For AWS S3, omit endpoint_url\n",
    "# s3_client = boto3.client('s3', endpoint_url='http://garage:3900')\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=S3_ENDPOINT_URL,\n",
    "    region_name=S3_REGION,\n",
    "    aws_access_key_id=S3_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=S3_SECRET_ACCESS_KEY,\n",
    "    config=Config(s3={\"addressing_style\": \"path\"}),\n",
    ")\n",
    "\n",
    "\n",
    "# Create store instance\n",
    "store = S3ExchangeStore(\n",
    "    s3_client=s3_client,\n",
    "    bucket=S3_BUCKET,\n",
    "    base_prefix='prod',  # Optional: prefix all keys\n",
    "    default_vars={'service_name': 'training-service'},\n",
    ")\n",
    "\n",
    "print(f\"Store initialized with bucket: {store.bucket}\")\n",
    "print(f\"Base prefix: {store.base_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Templating\n",
    "\n",
    "The library supports templating with placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved key: prod/training/123/samples\n",
      "Scoped key: prod/training/123/samples\n"
     ]
    }
   ],
   "source": [
    "# Resolve template with variables\n",
    "key = store._resolve_key(\"training/{job_id}/samples\", {\"job_id\": \"123\"})\n",
    "print(f\"Resolved key: {key}\")\n",
    "\n",
    "# Create a scoped store for convenience\n",
    "scoped = store.scope(job_id=\"123\", prefix=\"training\")\n",
    "key = scoped.key(\"{prefix}/{job_id}/samples\")\n",
    "print(f\"Scoped key: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Object Operations {#basic-ops}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created entry: {'kind': 'file', 'key': 'training/123/samples/file.wav', 'id': 'file-001', 'meta': {'sr': 16000, 'length': 15342}, 'content_type': 'audio/wav', 'size_bytes': 18, 'etag': '548baa8d1efffaaae5d59f4bedcf09b3'}\n",
      "Entry kind: file\n",
      "Entry key: training/123/samples/file.wav\n",
      "Entry metadata: {'sr': 16000, 'length': 15342}\n"
     ]
    }
   ],
   "source": [
    "# Put a single object with metadata\n",
    "entry = store.put_object(\n",
    "    key=\"training/123/samples/file.wav\",\n",
    "    data=b\"fake audio data...\",\n",
    "    id=\"file-001\",\n",
    "    meta={\"sr\": 16000, \"length\": 15342},\n",
    "    content_type=\"audio/wav\",\n",
    ")\n",
    "\n",
    "print(f\"Created entry: {entry}\")\n",
    "print(f\"Entry kind: {entry['kind']}\")\n",
    "print(f\"Entry key: {entry['key']}\")\n",
    "print(f\"Entry metadata: {entry.get('meta', {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = store.put_object(\n",
    "    key=\"training/123/samples/file1.wav\",\n",
    "    data=b\"fake audio data...\",\n",
    "    id=\"file-001\",\n",
    "    meta={\"sr\": 16000, \"length\": 15342},\n",
    "    content_type=\"audio/wav\",\n",
    ")\n",
    "\n",
    "entry = store.put_object(\n",
    "    key=\"training/123/samples/file2.wav\",\n",
    "    data=b\"fake audio data...\",\n",
    "    id=\"file-001\",\n",
    "    meta={\"sr\": 16000, \"length\": 15342},\n",
    "    content_type=\"audio/wav\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 18 bytes\n"
     ]
    }
   ],
   "source": [
    "# Read a single object\n",
    "stream = store.get_object(\"training/123/samples/file.wav\")\n",
    "data = stream.read()\n",
    "print(f\"Read {len(data)} bytes\")\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manifest Operations {#manifests}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Manifests\n",
    "\n",
    "Manifests track collections of objects. You can write in `overwrite` or `append_parts` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest written successfully\n"
     ]
    }
   ],
   "source": [
    "# Create initial manifest entries\n",
    "entries = [\n",
    "    {\"kind\": \"file\", \"key\": \"training/123/samples/file1.wav\", \"id\": \"001\"},\n",
    "    {\"kind\": \"file\", \"key\": \"training/123/samples/file2.wav\", \"id\": \"002\"},\n",
    "]\n",
    "\n",
    "# Write manifest in overwrite mode\n",
    "store.write_manifest(\n",
    "    key=\"training/123/samples/manifest.jsonl\",\n",
    "    entries=entries,\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "print(\"Manifest written successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append new entries using append_parts mode (recommended for updates)\n",
    "new_entries = [\n",
    "    {\"kind\": \"file\", \"key\": \"training/123/samples/file3.wav\", \"id\": \"003\"},\n",
    "]\n",
    "\n",
    "store.write_manifest(\n",
    "    key=\"training/123/samples/manifest.jsonl\",\n",
    "    entries=new_entries,\n",
    "    mode=\"append_parts\",  # Creates a part file and updates root manifest\n",
    ")\n",
    "\n",
    "print(\"New entries appended to manifest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest entries:\n",
      "  - file: training/123/samples/file1.wav\n",
      "  - file: training/123/samples/file2.wav\n"
     ]
    }
   ],
   "source": [
    "# Iterate over manifest entries without fetching objects\n",
    "print(\"Manifest entries:\")\n",
    "for entry in store.iter_manifest_entries(\"training/123/samples/manifest.jsonl\"):\n",
    "    print(f\"  - {entry['kind']}: {entry.get('key', entry.get('archive_key', 'N/A'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading objects from manifest:\n",
      "  Reading training/123/samples/file1.wav (id: 001)\n",
      "    Size: 18 bytes\n",
      "  Reading training/123/samples/file2.wav (id: 002)\n",
      "    Size: 18 bytes\n"
     ]
    }
   ],
   "source": [
    "# Iterate over objects in manifest (lazy iteration)\n",
    "print(\"Reading objects from manifest:\")\n",
    "for stream, entry in store.iter_objects(\"training/123/samples/manifest.jsonl\"):\n",
    "    print(f\"  Reading {entry['key']} (id: {entry.get('id', 'N/A')})\")\n",
    "    data = stream.read()\n",
    "    print(f\"    Size: {len(data)} bytes\")\n",
    "    stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Shard Archives {#shards}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shard archives are tar/tar.gz files containing multiple files with an internal manifest. They're useful for efficiently storing many small files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test files in: /tmp/tmpmazylr0x\n",
      "  Created file_000.txt (1800 bytes)\n",
      "  Created file_001.txt (1800 bytes)\n",
      "  Created file_002.txt (1800 bytes)\n",
      "  Created file_003.txt (1800 bytes)\n",
      "  Created file_004.txt (1800 bytes)\n"
     ]
    }
   ],
   "source": [
    "from s3_exchange import create_shards\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create some temporary files for demonstration\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "print(f\"Creating test files in: {temp_dir}\")\n",
    "\n",
    "# Create sample files\n",
    "for i in range(5):\n",
    "    file_path = temp_dir / f\"file_{i:03d}.txt\"\n",
    "    file_path.write_text(f\"Content of file {i}\\n\" * 100)\n",
    "    print(f\"  Created {file_path.name} ({file_path.stat().st_size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 shard batch(es)\n",
      "  Shard 0: 3 items\n",
      "  Shard 1: 2 items\n",
      "[[{'source': '/tmp/tmpmazylr0x/file_000.txt', 'member_path': 'file_000.txt', 'id': 'file-000', 'meta': {'index': 0, 'type': 'text'}, 'size_bytes': 1800}, {'source': '/tmp/tmpmazylr0x/file_001.txt', 'member_path': 'file_001.txt', 'id': 'file-001', 'meta': {'index': 1, 'type': 'text'}, 'size_bytes': 1800}, {'source': '/tmp/tmpmazylr0x/file_002.txt', 'member_path': 'file_002.txt', 'id': 'file-002', 'meta': {'index': 2, 'type': 'text'}, 'size_bytes': 1800}], [{'source': '/tmp/tmpmazylr0x/file_003.txt', 'member_path': 'file_003.txt', 'id': 'file-003', 'meta': {'index': 3, 'type': 'text'}, 'size_bytes': 1800}, {'source': '/tmp/tmpmazylr0x/file_004.txt', 'member_path': 'file_004.txt', 'id': 'file-004', 'meta': {'index': 4, 'type': 'text'}, 'size_bytes': 1800}]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare items for sharding\n",
    "items = [\n",
    "    {\n",
    "        \"source\": str(temp_dir / f\"file_{i:03d}.txt\"),\n",
    "        \"member_path\": f\"file_{i:03d}.txt\",\n",
    "        \"id\": f\"file-{i:03d}\",\n",
    "        \"meta\": {\"index\": i, \"type\": \"text\"},\n",
    "        \"size_bytes\": (temp_dir / f\"file_{i:03d}.txt\").stat().st_size,\n",
    "    }\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "# Split into shards (max 3 entries or 1MB per shard for demo)\n",
    "shard_batches = list(create_shards(\n",
    "    items,\n",
    "    max_entries=3,\n",
    "    max_bytes=1024 * 1024,  # 1 MB\n",
    "))\n",
    "\n",
    "print(f\"Created {len(shard_batches)} shard batch(es)\")\n",
    "for i, batch in enumerate(list(shard_batches)):\n",
    "    print(f\"  Shard {i}: {len(batch)} items\")\n",
    "\n",
    "print(list(shard_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded shard: training/123/samples/shards/shard-00000.tar.gz\n",
      "  Entry count: 3\n",
      "  Size: 296 bytes\n",
      "Uploaded shard: training/123/samples/shards/shard-00001.tar.gz\n",
      "  Entry count: 2\n",
      "  Size: 270 bytes\n"
     ]
    }
   ],
   "source": [
    "# Upload each shard\n",
    "shard_entries = []\n",
    "for i, batch in enumerate(shard_batches):\n",
    "    archive_key = f\"training/123/samples/shards/shard-{i:05d}.tar.gz\"\n",
    "    shard_entry = store.put_shard_archive(\n",
    "        archive_key=archive_key,\n",
    "        shard_items=batch,\n",
    "        format=\"tar\",\n",
    "        compression=\"gzip\",\n",
    "    )\n",
    "    shard_entries.append(shard_entry)\n",
    "    print(f\"Uploaded shard: {archive_key}\")\n",
    "    print(f\"  Entry count: {shard_entry.get('count', 0)}\")\n",
    "    print(f\"  Size: {shard_entry.get('size_bytes', 0)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard entries added to manifest\n"
     ]
    }
   ],
   "source": [
    "# Write shard entries to manifest\n",
    "store.put_sharded(\n",
    "    manifest_key=\"training/123/samples/manifest.jsonl\",\n",
    "    shard_entries=shard_entries,\n",
    "    update_mode=\"append_parts\",\n",
    ")\n",
    "\n",
    "print(\"Shard entries added to manifest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Shards\n",
    "\n",
    "Shards are automatically expanded when iterating objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ManifestWriter - Incremental Writing {#manifest-writer}\n",
    "\n",
    "The `ManifestWriter` allows you to write manifests incrementally, automatically flushing parts to S3 and managing shard archives. This is ideal for large-scale data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage with Loose Files\n",
    "\n",
    "Write manifest entries incrementally. The writer buffers entries locally and flushes them as part manifests when thresholds are exceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer stats: {'part_entries': 3, 'part_bytes': 485, 'parts_uploaded': 0, 'shard_entries': 0, 'shard_bytes': 0, 'shards_created': 0, 'is_closed': False}\n",
      "\n",
      "Manifest written: training/123/samples/manifest-writer.jsonl\n",
      "\n",
      "Manifest entries:\n",
      "  - file: training/123/samples/file1.wav\n",
      "  - file: training/123/samples/new_file1.wav\n",
      "  - file: training/123/samples/new_file2.wav\n"
     ]
    }
   ],
   "source": [
    "from s3_exchange import ManifestWriter\n",
    "\n",
    "# Open a manifest writer (use as context manager)\n",
    "manifest_key = \"training/123/samples/manifest-writer.jsonl\"\n",
    "\n",
    "with store.open_manifest_writer(\n",
    "    manifest_key,\n",
    "    mode=\"append_parts\",  # Flush parts incrementally\n",
    "    part_max_entries=10,  # Small threshold for demo\n",
    "    part_max_bytes=1024,  # Small threshold for demo\n",
    ") as writer:\n",
    "    # Add existing files (assumes objects already exist in S3)\n",
    "    writer.add_file(\n",
    "        key=\"training/123/samples/file1.wav\",\n",
    "        id=\"file-001\",\n",
    "        meta={\"sr\": 16000},\n",
    "    )\n",
    "    \n",
    "    # Put new objects and add to manifest\n",
    "    writer.put_object(\n",
    "        key=\"training/123/samples/new_file1.wav\",\n",
    "        data=b\"new audio data 1\",\n",
    "        id=\"new-001\",\n",
    "        meta={\"sr\": 16000},\n",
    "    )\n",
    "    \n",
    "    writer.put_object(\n",
    "        key=\"training/123/samples/new_file2.wav\",\n",
    "        data=b\"new audio data 2\",\n",
    "        id=\"new-002\",\n",
    "        meta={\"sr\": 16000},\n",
    "    )\n",
    "    \n",
    "    # Check stats\n",
    "    stats = writer.stats()\n",
    "    print(f\"Writer stats: {stats}\")\n",
    "\n",
    "# On exit, root manifest is automatically updated with part references\n",
    "print(f\"\\nManifest written: {manifest_key}\")\n",
    "\n",
    "# Verify manifest\n",
    "print(\"\\nManifest entries:\")\n",
    "for entry in store.iter_manifest_entries(manifest_key):\n",
    "    print(f\"  - {entry['kind']}: {entry.get('key', entry.get('archive_key', 'N/A'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ManifestWriter with Shards\n",
    "\n",
    "The writer can automatically create shard archives when shard size policies are configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test files in: /tmp/tmpze3edvqy\n",
      "  Created shard_file_000.txt (1200 bytes)\n",
      "  Created shard_file_001.txt (1200 bytes)\n",
      "  Created shard_file_002.txt (1200 bytes)\n",
      "  Created shard_file_003.txt (1200 bytes)\n",
      "  Created shard_file_004.txt (1200 bytes)\n",
      "  Created shard_file_005.txt (1200 bytes)\n",
      "  Created shard_file_006.txt (1200 bytes)\n",
      "  Created shard_file_007.txt (1200 bytes)\n",
      "\n",
      "Writer stats: {'part_entries': 3, 'part_bytes': 643, 'parts_uploaded': 0, 'shard_entries': 2, 'shard_bytes': 2400, 'shards_created': 2, 'is_closed': False}\n",
      "  Shards created: 2\n",
      "  Shard entries: 2\n",
      "\n",
      "Manifest entries in training/123/samples/manifest-writer-shards.jsonl:\n",
      "  - shard: training/123/samples/shards/shard-000001.tar (3 items)\n",
      "  - shard: training/123/samples/shards/shard-000002.tar (3 items)\n",
      "  - file: training/123/samples/loose_file.wav\n",
      "  - shard: training/123/samples/shards/shard-000003.tar (2 items)\n"
     ]
    }
   ],
   "source": [
    "from s3_exchange import ShardSizePolicy\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary files for sharding demo\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "print(f\"Creating test files in: {temp_dir}\")\n",
    "\n",
    "for i in range(8):\n",
    "    file_path = temp_dir / f\"shard_file_{i:03d}.txt\"\n",
    "    file_path.write_text(f\"Content of shard file {i}\\n\" * 50)\n",
    "    print(f\"  Created {file_path.name} ({file_path.stat().st_size} bytes)\")\n",
    "\n",
    "# Open writer with shard support\n",
    "manifest_key_shards = \"training/123/samples/manifest-writer-shards.jsonl\"\n",
    "\n",
    "with store.open_manifest_writer(\n",
    "    manifest_key_shards,\n",
    "    mode=\"overwrite\",\n",
    "    shard_size=ShardSizePolicy(\n",
    "        max_entries=3,  # Flush shard after 3 items\n",
    "        max_bytes=5000,  # Or after 5KB\n",
    "    ),\n",
    "    shard_format=\"tar\",\n",
    "    # shard_compression=\"gzip\",\n",
    "    shard_compression=None,\n",
    ") as writer:\n",
    "    # Add files to shards\n",
    "    for i in range(8):\n",
    "        writer.add_to_shard(\n",
    "            member_path=f\"shard_file_{i:03d}.txt\",\n",
    "            source=str(temp_dir / f\"shard_file_{i:03d}.txt\"),\n",
    "            id=f\"shard-{i:03d}\",\n",
    "            meta={\"index\": i, \"type\": \"text\"},\n",
    "        )\n",
    "    \n",
    "    # Can also mix loose files and shards\n",
    "    writer.put_object(\n",
    "        key=\"training/123/samples/loose_file.wav\",\n",
    "        data=b\"loose audio data\",\n",
    "        id=\"loose-001\",\n",
    "    )\n",
    "    \n",
    "    stats = writer.stats()\n",
    "    print(f\"\\nWriter stats: {stats}\")\n",
    "    print(f\"  Shards created: {stats['shards_created']}\")\n",
    "    print(f\"  Shard entries: {stats['shard_entries']}\")\n",
    "\n",
    "# Cleanup temp files\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "# Verify manifest\n",
    "print(f\"\\nManifest entries in {manifest_key_shards}:\")\n",
    "for entry in store.iter_manifest_entries(manifest_key_shards):\n",
    "    kind = entry['kind']\n",
    "    if kind == 'shard':\n",
    "        print(f\"  - {kind}: {entry.get('archive_key')} ({entry.get('count', 0)} items)\")\n",
    "    elif kind == 'file':\n",
    "        print(f\"  - {kind}: {entry.get('key')}\")\n",
    "    elif kind == 'manifest_ref':\n",
    "        print(f\"  - {kind}: {entry.get('key')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - __manifest__.jsonl\n",
      "  - shard_file_000.txt\n",
      "  - shard_file_001.txt\n",
      "  - shard_file_002.txt\n",
      "b'{\"kind\":\"file\",\"key\":\"training/123/samples/shards/shard-000001.tar#shard_file_000.txt\",\"member_path\":\"shard_file_000.txt\",\"id\":\"shard-000\",\"meta\":{\"index\":0,\"type\":\"text\"},\"size_bytes\":1200}\\n{\"kind\":\"file\",\"key\":\"training/123/samples/shards/shard-000001.tar#shard_file_001.txt\",\"member_path\":\"shard_file_001.txt\",\"id\":\"shard-001\",\"meta\":{\"index\":1,\"type\":\"text\"},\"size_bytes\":1200}\\n{\"kind\":\"file\",\"key\":\"training/123/samples/shards/shard-000001.tar#shard_file_002.txt\",\"member_path\":\"shard_file_002.txt\",\"id\":\"shard-002\",\"meta\":{\"index\":2,\"type\":\"text\"},\"size_bytes\":1200}\\n'\n"
     ]
    }
   ],
   "source": [
    "# Read tar file from training/123/samples/shards/shard-000001.tar and iter the existing file names\n",
    "stream = store.get_object(\"training/123/samples/shards/shard-000001.tar\")\n",
    "\n",
    "raw_data = stream.read()\n",
    "\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "# Read the tar file from the raw data\n",
    "tar = tarfile.open(fileobj=io.BytesIO(raw_data))\n",
    "\n",
    "# Iter the existing file names\n",
    "for member in tar.getmembers():\n",
    "    print(f\"  - {member.name}\")\n",
    "\n",
    "# Print the content of __manifest__.jsonl\n",
    "manifest_data = tar.extractfile(\"__manifest__.jsonl\").read()\n",
    "print(manifest_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ExFileObject name='__manifest__.jsonl'> <class 'tarfile.ExFileObject'>\n",
      "  - shard-000\n",
      "    Size: 1200 bytes\n",
      "  - shard-001\n",
      "    Size: 1200 bytes\n",
      "  - shard-002\n",
      "    Size: 1200 bytes\n",
      "<ExFileObject name='__manifest__.jsonl'> <class 'tarfile.ExFileObject'>\n",
      "  - shard-003\n",
      "    Size: 1200 bytes\n",
      "  - shard-004\n",
      "    Size: 1200 bytes\n",
      "  - shard-005\n",
      "    Size: 1200 bytes\n",
      "  - loose-001\n",
      "    Size: 16 bytes\n",
      "<ExFileObject name='__manifest__.jsonl'> <class 'tarfile.ExFileObject'>\n",
      "  - shard-006\n",
      "    Size: 1200 bytes\n",
      "  - shard-007\n",
      "    Size: 1200 bytes\n"
     ]
    }
   ],
   "source": [
    "# Read from manitest\n",
    "for stream, entry in store.iter_objects(manifest_key_shards):\n",
    "# for stream, entry in store.iter_objects(\"training/123/samples/shards/shard-000002.tar\"):\n",
    "    # print(f\"  - {entry['kind']}: {entry['key']}\")\n",
    "    print(f\"  - {entry[\"id\"]}\")\n",
    "    data = stream.read()\n",
    "    print(f\"    Size: {len(data)} bytes\")\n",
    "    stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Content of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\nContent of shard file 0\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get_object(\"training/123/samples/shards/shard-000001.tar#shard_file_000.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwrite Mode\n",
    "\n",
    "In overwrite mode, all entries are buffered and written as a single manifest on close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_key_overwrite = \"training/123/samples/manifest-writer-overwrite.jsonl\"\n",
    "\n",
    "with store.open_manifest_writer(\n",
    "    manifest_key_overwrite,\n",
    "    mode=\"overwrite\",  # Single manifest on close\n",
    "    part_max_entries=10,  # Ignored in overwrite mode\n",
    ") as writer:\n",
    "    for i in range(5):\n",
    "        writer.put_object(\n",
    "            key=f\"training/123/samples/overwrite_file_{i}.wav\",\n",
    "            data=f\"data {i}\".encode(),\n",
    "            id=f\"overwrite-{i:03d}\",\n",
    "        )\n",
    "    \n",
    "    stats = writer.stats()\n",
    "    print(f\"Entries written: {stats['part_entries']}\")\n",
    "    print(f\"Parts uploaded: {stats['parts_uploaded']}\")  # Should be 0 in overwrite mode\n",
    "\n",
    "print(f\"\\nManifest written: {manifest_key_overwrite}\")\n",
    "\n",
    "# Verify - should be a single manifest file (no parts)\n",
    "print(\"\\nManifest entries:\")\n",
    "for entry in store.iter_manifest_entries(manifest_key_overwrite, resolve_refs=False):\n",
    "    print(f\"  - {entry['kind']}: {entry.get('key', entry.get('archive_key', 'N/A'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling\n",
    "\n",
    "By default, if an exception occurs, the root manifest is not published (preventing partial/incomplete manifests). You can enable `publish_on_error=True` for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_key_error = \"training/123/samples/manifest-writer-error.jsonl\"\n",
    "\n",
    "# Default behavior: don't publish on error\n",
    "try:\n",
    "    with store.open_manifest_writer(manifest_key_error) as writer:\n",
    "        writer.put_object(\n",
    "            key=\"training/123/samples/error_file.wav\",\n",
    "            data=b\"data\",\n",
    "            id=\"error-001\",\n",
    "        )\n",
    "        # Simulate an error\n",
    "        raise ValueError(\"Something went wrong!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Caught error: {e}\")\n",
    "\n",
    "# Check if manifest exists (should not exist - not published)\n",
    "exists = store.exists(manifest_key_error)\n",
    "print(f\"Manifest exists after error: {exists}\")\n",
    "\n",
    "# With publish_on_error=True, manifest would be published even on error\n",
    "manifest_key_error_publish = \"training/123/samples/manifest-writer-error-publish.jsonl\"\n",
    "try:\n",
    "    with store.open_manifest_writer(\n",
    "        manifest_key_error_publish,\n",
    "        publish_on_error=True,  # Publish even on error\n",
    "    ) as writer:\n",
    "        writer.put_object(\n",
    "            key=\"training/123/samples/error_file_publish.wav\",\n",
    "            data=b\"data\",\n",
    "            id=\"error-publish-001\",\n",
    "        )\n",
    "        raise ValueError(\"Something went wrong!\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\nCaught error: {e}\")\n",
    "\n",
    "# Check if manifest exists (should exist - published on error)\n",
    "exists = store.exists(manifest_key_error_publish)\n",
    "print(f\"Manifest exists after error (publish_on_error=True): {exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Flush Control\n",
    "\n",
    "You can manually flush parts or shards if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_key_manual = \"training/123/samples/manifest-writer-manual.jsonl\"\n",
    "\n",
    "with store.open_manifest_writer(\n",
    "    manifest_key_manual,\n",
    "    mode=\"append_parts\",\n",
    "    part_max_entries=100,  # High threshold - won't auto-flush\n",
    ") as writer:\n",
    "    # Add some entries\n",
    "    for i in range(5):\n",
    "        writer.put_object(\n",
    "            key=f\"training/123/samples/manual_file_{i}.wav\",\n",
    "            data=f\"data {i}\".encode(),\n",
    "            id=f\"manual-{i:03d}\",\n",
    "        )\n",
    "    \n",
    "    # Manually flush part\n",
    "    part_key = writer.flush_part()\n",
    "    if part_key:\n",
    "        print(f\"Manually flushed part: {part_key}\")\n",
    "    \n",
    "    # Add more entries\n",
    "    for i in range(5, 10):\n",
    "        writer.put_object(\n",
    "            key=f\"training/123/samples/manual_file_{i}.wav\",\n",
    "            data=f\"data {i}\".encode(),\n",
    "            id=f\"manual-{i:03d}\",\n",
    "        )\n",
    "    \n",
    "    stats = writer.stats()\n",
    "    print(f\"\\nFinal stats: {stats}\")\n",
    "    print(f\"  Parts uploaded: {stats['parts_uploaded']}\")\n",
    "\n",
    "print(f\"\\nManifest written: {manifest_key_manual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shards are automatically expanded when iterating objects\n",
    "print(\"Reading from shards:\")\n",
    "for stream, entry in store.iter_objects(\"training/123/samples/manifest.jsonl\"):\n",
    "    if entry.get('archive_key'):\n",
    "        # This is a shard member\n",
    "        print(f\"  Shard member: {entry['member_path']}\")\n",
    "        print(f\"    Archive: {entry['archive_key']}\")\n",
    "        print(f\"    Virtual key: {entry['key']}\")\n",
    "    else:\n",
    "        # Regular file\n",
    "        print(f\"  File: {entry['key']}\")\n",
    "    \n",
    "    data = stream.read()\n",
    "    print(f\"    Size: {len(data)} bytes\")\n",
    "    stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Listing and Filtering {#listing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List S3 keys\n",
    "print(\"S3 keys with prefix 'training/123/':\")\n",
    "for key in store.list_keys(prefix=\"training/123/\"):\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in manifest (including shard members)\n",
    "print(\"Files in manifest:\")\n",
    "for entry in store.list_manifest_files(\n",
    "    manifest=\"training/123/samples/manifest.jsonl\",\n",
    "    include_shards=True,\n",
    "):\n",
    "    key = entry.get('key', entry.get('member_path', 'N/A'))\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter manifest entries by prefix\n",
    "print(\"Filtered entries (prefix 'training/123/samples/shards'):\")\n",
    "for entry in store.list_by_manifest_prefix(\n",
    "    manifest=\"training/123/samples/manifest.jsonl\",\n",
    "    prefix_filter=\"training/123/samples/shards\",\n",
    "):\n",
    "    print(f\"  {entry.get('key', entry.get('archive_key', 'N/A'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deletion Operations {#deletion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a single object\n",
    "# Note: Uncomment to actually delete\n",
    "# store.delete_key(\"training/123/samples/file.wav\")\n",
    "# print(\"Deleted single object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete by prefix (with optional regex filter)\n",
    "# Note: This is commented out to avoid deleting all demo data\n",
    "# count = store.delete_prefix(\n",
    "#     prefix=\"training/123/samples/\",\n",
    "#     regex=r\".*\\.wav$\",  # Only delete .wav files\n",
    "# )\n",
    "# print(f\"Deleted {count} objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete by manifest (recursive)\n",
    "# Note: This is commented out to avoid deleting all demo data\n",
    "# report = store.delete_manifest(\n",
    "#     manifest=\"training/123/samples/manifest.jsonl\",\n",
    "#     delete_manifests=True,  # Also delete manifest files\n",
    "#     dedupe=True,  # Avoid double-deletion\n",
    "# )\n",
    "# print(f\"Deleted {report['deleted_object_count']} objects\")\n",
    "# print(f\"Deleted {report['deleted_archive_count']} archives\")\n",
    "# print(f\"Deleted {report['deleted_manifest_count']} manifests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling {#errors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3_exchange import (\n",
    "    ObjectNotFoundError,\n",
    "    ManifestNotFoundError,\n",
    "    MissingPlaceholderError,\n",
    "    InvalidManifestError,\n",
    "    ShardReadError,\n",
    ")\n",
    "\n",
    "# Handle object not found\n",
    "try:\n",
    "    stream = store.get_object(\"nonexistent.wav\")\n",
    "except ObjectNotFoundError as e:\n",
    "    print(f\"Object not found: {e.key}\")\n",
    "\n",
    "# Handle missing placeholder\n",
    "try:\n",
    "    key = store._resolve_key(\"training/{job_id}/samples\", {})  # Missing job_id\n",
    "except MissingPlaceholderError as e:\n",
    "    print(f\"Missing placeholder: {e.placeholder}\")\n",
    "\n",
    "# Handle manifest not found\n",
    "try:\n",
    "    for entry in store.iter_manifest_entries(\"nonexistent/manifest.jsonl\"):\n",
    "        pass\n",
    "except ManifestNotFoundError as e:\n",
    "    print(f\"Manifest not found: {e.manifest_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Manifest Compaction\n",
    "\n",
    "Flatten a manifest with many parts into a single clean manifest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact manifest (combines all parts into one)\n",
    "# Note: This is commented out to avoid modifying demo data\n",
    "# report = store.compact_manifest(\n",
    "#     src_manifest_key=\"training/123/samples/manifest.jsonl\",\n",
    "#     dst_manifest_key=\"training/123/samples/manifest-compact.jsonl\",\n",
    "#     resolve_refs=True,  # Resolve all manifest_ref entries\n",
    "#     expand_shards=False,  # Keep shard entries (set True to expand into files)\n",
    "# )\n",
    "# print(f\"Compacted {report['total_entries']} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✅ Store initialization and key templating\n",
    "- ✅ Basic object read/write operations\n",
    "- ✅ Manifest creation and reading\n",
    "- ✅ Shard archive creation and reading\n",
    "- ✅ ManifestWriter for incremental writing\n",
    "- ✅ Listing and filtering operations\n",
    "- ✅ Deletion operations\n",
    "- ✅ Error handling\n",
    "\n",
    "For more details, see the [README.md](../README.md) file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
